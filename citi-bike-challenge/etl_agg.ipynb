{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4341fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc5ed8a",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c60b732",
   "metadata": {},
   "source": [
    "## Station Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd02741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Citi Bike publishes real-time system data in GBFS format.\n",
    "# gbfs_response = requests.get('http://gbfs.citibikenyc.com/gbfs/gbfs.json').json()\n",
    "# station_information_json = requests.get(gbfs_response['data']['en']['feeds'][1]['url']).json()\n",
    "\n",
    "# # Export the station information as a json file to avoid calling real-time data everytime this notebook is run.\n",
    "# # Station info reflects the latest info upon calling the station API.  \n",
    "# with open(os.path.join('data', 'station_info.json'), 'w') as f:\n",
    "#     json.dump(station_information_json['data']['stations'], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545bc026",
   "metadata": {},
   "source": [
    "## Trip Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a072a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Much like extraction of the station information, the following code needs to be executed only once to extract the trip data\n",
    "# # of interest. The code has been written with downloading data for multiple months and combining them into one CSV file in mind.\n",
    "# base_url = \"https://s3.amazonaws.com/tripdata/\"\n",
    "# df_li = []\n",
    "\n",
    "# # Limit the scope of the study to June 2022.\n",
    "# year = '2022'\n",
    "# # Loop through the period of interest by month. For a whole year of data, specify np.arange(1,13).\n",
    "# for i in np.arange(6,7):\n",
    "#     if i not in [6, 7]:\n",
    "#         csv_name = f'{year}{i:02d}-citibike-tripdata.csv'\n",
    "#     else:\n",
    "#         # The file names for June and July of 2022 are different from those for the other months due to typo.\n",
    "#         csv_name = f'{year}{i:02d}-citbike-tripdata.csv'\n",
    "    \n",
    "#     # The Citi Bike system data are stored as zip files.\n",
    "#     zip_name = csv_name + '.zip'\n",
    "#     zip_url = base_url + zip_name\n",
    "    \n",
    "#     # Request the zip file and extract its content.\n",
    "#     zip_response = requests.get(zip_url)\n",
    "#     with open(zip_name, 'wb') as f:\n",
    "#         f.write(zip_response.content)\n",
    "#     with zipfile.ZipFile(zip_name, 'r') as zip:\n",
    "#         zip.extractall(year)\n",
    "    \n",
    "#     # Import the extract CSV file and create a DataFrame for it.\n",
    "#     df = pd.read_csv(os.path.join(year, csv_name), dtype={'start_station_id': str, 'end_station_id': str})\n",
    "#     # 'ride_id' can be dropped immediately as it is used to identify trips, as do the DataFrame indices.\n",
    "#     df.drop(columns=['ride_id'], inplace=True)\n",
    "    \n",
    "#     df_li.append(df)\n",
    "#     os.remove(zip_name)\n",
    "#     os.remove(os.path.join(year, csv_name))\n",
    "    \n",
    "# # Concatenate all monthly data.\n",
    "# df = pd.concat(df_li, axis=0, ignore_index=True)\n",
    "\n",
    "# # Edited: import the additional JC (Jersey City) data for June 2022.\n",
    "# # Upon close investigation of 202206-citibike-tripdata.csv, it is noted that a few trips that ended in JC are included in the\n",
    "# # dataset. Because Citi Bike extends to Jersey City and Hoboken in New Jersey, the JC data, which share the same base URL with \n",
    "# # the NYC data, are included for the time period investigated.\n",
    "# csv_name = f'JC-{year}06-citibike-tripdata.csv'\n",
    "# zip_name = csv_name + '.zip'\n",
    "# zip_response = requests.get(base_url + zip_name)\n",
    "\n",
    "# with open(zip_name, 'wb') as f:\n",
    "#     f.write(zip_response.content)\n",
    "# with zipfile.ZipFile(zip_name, 'r') as zip:\n",
    "#     zip.extractall(year)\n",
    "    \n",
    "# df_jc = pd.read_csv(os.path.join(year, csv_name), dtype={'start_station_id': str, 'end_station_id': str})\n",
    "# df_jc.drop(columns=['ride_id'], inplace=True)\n",
    "\n",
    "# os.remove(zip_name)\n",
    "# os.remove(os.path.join(year, csv_name))\n",
    "\n",
    "# df = pd.concat([df, df_jc], axis=0, ignore_index=True)\n",
    "# df.to_csv(os.path.join(year, f'{year}06-citibike-tripdata.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba74d2",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a201bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ridership CSV file.\n",
    "df = pd.read_csv(os.path.join('2022', '202206-citibike-tripdata.csv'), dtype={'start_station_id': str, 'end_station_id': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93540363",
   "metadata": {},
   "source": [
    "## Trip Duration Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e19456d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3441935 entries, 0 to 3536181\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count    Dtype         \n",
      "---  ------              --------------    -----         \n",
      " 0   started_at          3441935 non-null  datetime64[ns]\n",
      " 1   ended_at            3441935 non-null  datetime64[ns]\n",
      " 2   start_station_name  3441935 non-null  object        \n",
      " 3   start_station_id    3441935 non-null  object        \n",
      " 4   end_station_name    3439610 non-null  object        \n",
      " 5   end_station_id      3439610 non-null  object        \n",
      " 6   start_lat           3441935 non-null  float64       \n",
      " 7   start_lng           3441935 non-null  float64       \n",
      " 8   end_lat             3441654 non-null  float64       \n",
      " 9   end_lng             3441654 non-null  float64       \n",
      " 10  user_type           3441935 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(4), object(5)\n",
      "memory usage: 315.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_1 = df.copy()\n",
    "\n",
    "# Convert start time and end time to datetime objects to calculate trip duration.\n",
    "df_1['started_at'] = pd.to_datetime(df_1['started_at'])\n",
    "df_1['ended_at'] = pd.to_datetime(df_1['ended_at'])\n",
    "df_1['trip_duration'] = (df_1['ended_at'] - df_1['started_at']).dt.total_seconds()\n",
    "\n",
    "# The dataset is still relatively new, so it still needs to be processed to remove trips below 60 seconds in length as per\n",
    "# https://citibikenyc.com/system-data. There are trips lasting days in the dataset, which are clearly outliers, so an upper\n",
    "# limit of one day is set as well.\n",
    "df_1 = df_1[(df_1['trip_duration']>=60) & (df_1['trip_duration']<=(60*60*24))]\n",
    "\n",
    "# Trip duration has been calculated to remove outliers and will be dropped to reduce the notebook workload. (It will be computed\n",
    "# again in Tableau.) The type of bikes used for each trip is outside the scope of the current study, so it will be dropped, too.\n",
    "df_1.drop(columns=['rideable_type', 'trip_duration'], inplace=True)\n",
    "\n",
    "# Rename 'member_casual' to 'user_type'.\n",
    "df_1.rename(columns={'member_casual': 'user_type'}, inplace=True)\n",
    "\n",
    "df_1.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd9a994",
   "metadata": {},
   "source": [
    "## Station Names and Ids\n",
    "To ensure accurate groupby operations downstream, each station name should be related to its id one-to-one. However, for both start and end stations, the number of unique names is greater than the number of unique ids, indicating multiple station names referring to the same id and possibly vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a7959",
   "metadata": {},
   "source": [
    "### Start Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78f4d422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique start station names: 1646\n",
      "Number of unique start station ids: 1638\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unique start station names: {df_1['start_station_name'].nunique()}\")\n",
    "print(f\"Number of unique start station ids: {df_1['start_station_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f30c9b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4488.09: 'Boerum Pl\\\\t& Pacific St', 'Boerum Pl\\t& Pacific St'\n",
      "4781.05: 'Nassau St\\\\t& Duffield St', 'Nassau St\\t& Duffield St'\n",
      "5323.06: 'Sharon St & Olive St', 'Sharon St & Olive St_new'\n",
      "5329.08: 'Murray St\\\\t& West St', 'Murray St\\t& West St'\n",
      "5382.07: 'Forsyth St\\t& Grand St', 'Forsyth St\\\\t& Grand St'\n",
      "5883.06: 'Van Dam St & Greenpoint Ave', 'Van Dam St & Review Ave'\n",
      "6300.04: 'Skillman Ave & 43 Ave', 'Skillman Ave & 32 Pl'\n",
      "6535.04: 'W 34 St &\\\\tHudson Blvd E', 'W 34 St &\\tHudson Blvd E'\n",
      "6560.14: 'W 40 St & 7 Ave', 'W 40 St & 8 Ave'\n",
      "6708.04: 'Broadway\\\\t& W 48 St', 'Broadway\\t& W 48 St'\n"
     ]
    }
   ],
   "source": [
    "# Determine the start station ids that multiple station names refer to.\n",
    "# If the station name is related to its id one-to-one, then df_1[['start_station_name', 'start_station_id']].drop_duplicates()\n",
    "# should have the same index sequence as df_1['start_station_id'].drop_duplicates(). However, that is not the case: the \n",
    "# additional station names need to be addressed.\n",
    "diff_id = df_1[['start_station_name', 'start_station_id']].drop_duplicates().index.difference(\n",
    "    df_1['start_station_id'].drop_duplicates().index)\n",
    "diff = df_1.loc[diff_id]['start_station_id'].sort_values()\n",
    "\n",
    "# Print the start station ids with their conflicting names.\n",
    "# repr() is used to prevent escape sequence interpretation.\n",
    "for i in diff:\n",
    "    print(i + ': ' + \n",
    "          ', '.join(repr(name) for name in df_1[df_1['start_station_id'] == i]['start_station_name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c8d80ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 6 of them, it is a simple matter of replacing '\\\\t' with '\\t' (and removing '\\t' altogether in station names), and the\n",
    "# rest can be adjusted using the station information as reference.\n",
    "df_1['start_station_name'] = df_1['start_station_name'].str.replace(r'\\\\t', r'\\t', regex=True)\n",
    "df_1['start_station_name'] = df_1['start_station_name'].str.replace(r'\\t', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "121f6a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5323.06: Sharon St & Olive St\n",
      "5883.06: Van Dam St & Greenpoint Ave\n",
      "6300.04: Skillman Ave & 43 Ave\n",
      "6560.14: W 40 St & 7 Ave\n"
     ]
    }
   ],
   "source": [
    "# Import the station information and use it as a reference to resolve the remaining conflicting station names.\n",
    "station_info = pd.read_json(os.path.join('data', 'station_info.json')).loc[:, ['short_name', 'name', 'lat', 'lon']]\n",
    "station_info.rename(columns={'short_name':'station_id'}, inplace=True)\n",
    "\n",
    "# Note that the list can be generated by the same code used to determine the start station ids that multiple station names \n",
    "# refer to.\n",
    "for i in ['5323.06', '5883.06', '6300.04', '6560.14']:\n",
    "    correct_name = station_info[station_info['station_id'] == i].name.str.cat()\n",
    "    print(i + ': ' + correct_name)\n",
    "    df_1.loc[df_1['start_station_id'] == i, 'start_station_name'] = correct_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca8c3e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 Ave & 12 St: 7034.08, 7034.09\n",
      "Sharon St & Olive St: 5323.05, 5323.06\n"
     ]
    }
   ],
   "source": [
    "# Determine the start station names that multiple station ids refer to.\n",
    "diff_id = df_1[['start_station_name', 'start_station_id']].drop_duplicates().index.difference(\n",
    "    df_1['start_station_name'].drop_duplicates().index)\n",
    "diff = df_1.loc[diff_id]['start_station_name'].sort_values()\n",
    "\n",
    "# Print the start station names and their conflicting ids.\n",
    "for name in diff:\n",
    "    print(name + ': ' + ', '.join(i for i in df_1[df_1['start_station_name'] == name]['start_station_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e8257bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 Ave & 12 St: 7034.09\n",
      "Sharon St & Olive St: 5323.06\n"
     ]
    }
   ],
   "source": [
    "# Once again, use the station information to resolve the conflicting station ids.\n",
    "for name in diff:\n",
    "    correct_id = station_info[station_info['name'] == name]['station_id'].str.cat()\n",
    "    print(name + ': ' + correct_id)\n",
    "    df_1.loc[df_1['start_station_name'] == name, 'start_station_id'] = correct_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95b72be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique start station names: 1636\n",
      "Number of unique start station ids: 1636\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unique start station names: {df_1['start_station_name'].nunique()}\")\n",
    "print(f\"Number of unique start station ids: {df_1['start_station_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1d41f",
   "metadata": {},
   "source": [
    "### End Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3c2dbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique end station names: 1650\n",
      "Number of unique end station ids: 1642\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unique end station names: {df_1['end_station_name'].nunique()}\")\n",
    "print(f\"Number of unique end station ids: {df_1['end_station_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f400052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4488.09: 'Boerum Pl\\\\t& Pacific St', 'Boerum Pl\\t& Pacific St'\n",
      "4781.05: 'Nassau St\\\\t& Duffield St', 'Nassau St\\t& Duffield St'\n",
      "5323.06: 'Sharon St & Olive St', 'Sharon St & Olive St_new'\n",
      "5329.08: 'Murray St\\\\t& West St', 'Murray St\\t& West St'\n",
      "5382.07: 'Forsyth St\\\\t& Grand St', 'Forsyth St\\t& Grand St'\n",
      "5883.06: 'Van Dam St & Review Ave', 'Van Dam St & Greenpoint Ave'\n",
      "6300.04: 'Skillman Ave & 43 Ave', 'Skillman Ave & 32 Pl'\n",
      "6535.04: 'W 34 St &\\\\tHudson Blvd E', 'W 34 St &\\tHudson Blvd E'\n",
      "6560.14: 'W 40 St & 7 Ave', 'W 40 St & 8 Ave'\n",
      "6708.04: 'Broadway\\\\t& W 48 St', 'Broadway\\t& W 48 St'\n"
     ]
    }
   ],
   "source": [
    "# Determine the end station ids that multiple station names refer to.\n",
    "diff_id = df_1[['end_station_name', 'end_station_id']].drop_duplicates().index.difference(\n",
    "    df_1['end_station_id'].drop_duplicates().index)\n",
    "diff = df_1.loc[diff_id]['end_station_id'].sort_values()\n",
    "\n",
    "# Print the end station ids with their conflicting names.\n",
    "for i in diff:\n",
    "    print(i + ': ' + \n",
    "          ', '.join(repr(name) for name in df_1[df_1['end_station_id'] == i]['end_station_name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a979634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting end station ids are the same as the start station ids identified above, so the conflicting names can be resolved\n",
    "# with the same approach.\n",
    "df_1['end_station_name'] = df_1['end_station_name'].str.replace(r'\\\\t', r'\\t', regex=True)\n",
    "df_1['end_station_name'] = df_1['end_station_name'].str.replace(r'\\t', ' ', regex=True)\n",
    "\n",
    "for i in ['5323.06', '5883.06', '6300.04', '6560.14']:\n",
    "    correct_name = station_info[station_info['station_id'] == i].name.str.cat()\n",
    "    df_1.loc[df_1['end_station_id'] == i, 'end_station_name'] = correct_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2440f277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 Ave & 12 St: 7034.09, 7034.08\n",
      "Sharon St & Olive St: 5323.06, 5323.05\n"
     ]
    }
   ],
   "source": [
    "# Determine the end station names that multiple station ids refer to.\n",
    "diff_id = df_1[['end_station_name', 'end_station_id']].drop_duplicates().index.difference(\n",
    "    df_1['end_station_name'].drop_duplicates().index)\n",
    "diff = df_1.loc[diff_id]['end_station_name'].sort_values()\n",
    "\n",
    "# Print the end station names and their conflicting ids.\n",
    "for name in diff:\n",
    "    print(name + ': ' + ', '.join(i for i in df_1[df_1['end_station_name'] == name]['end_station_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebf3c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting end station names are the same as the start station names identified above, so the conflicting ids can be\n",
    "# resolved with the same approach.\n",
    "for name in diff:\n",
    "    correct_id = station_info[station_info['name'] == name]['station_id'].str.cat()\n",
    "    df_1.loc[df_1['end_station_name'] == name, 'end_station_id'] = correct_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f1bda0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique end station names: 1640\n",
      "Number of unique end station ids: 1640\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unique end station names: {df_1['end_station_name'].nunique()}\")\n",
    "print(f\"Number of unique end station ids: {df_1['end_station_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e97e5c",
   "metadata": {},
   "source": [
    "## Station Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8a64c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320718</th>\n",
       "      <td>40.717798</td>\n",
       "      <td>-73.993161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337173</th>\n",
       "      <td>40.717798</td>\n",
       "      <td>-73.993161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369955</th>\n",
       "      <td>40.717444</td>\n",
       "      <td>-73.993426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386524</th>\n",
       "      <td>40.717684</td>\n",
       "      <td>-73.993301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408448</th>\n",
       "      <td>40.717780</td>\n",
       "      <td>-73.993254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266132</th>\n",
       "      <td>40.717781</td>\n",
       "      <td>-73.993242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3286564</th>\n",
       "      <td>40.717516</td>\n",
       "      <td>-73.993466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3395379</th>\n",
       "      <td>40.717567</td>\n",
       "      <td>-73.993388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428005</th>\n",
       "      <td>40.717710</td>\n",
       "      <td>-73.993271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428036</th>\n",
       "      <td>40.717422</td>\n",
       "      <td>-73.993429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         start_lat  start_lng\n",
       "320718   40.717798 -73.993161\n",
       "337173   40.717798 -73.993161\n",
       "369955   40.717444 -73.993426\n",
       "386524   40.717684 -73.993301\n",
       "408448   40.717780 -73.993254\n",
       "...            ...        ...\n",
       "3266132  40.717781 -73.993242\n",
       "3286564  40.717516 -73.993466\n",
       "3395379  40.717567 -73.993388\n",
       "3428005  40.717710 -73.993271\n",
       "3428036  40.717422 -73.993429\n",
       "\n",
       "[281 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = df_1.copy()\n",
    "\n",
    "# Inspect the starting latitude and longitude for trips started from 5382.07.\n",
    "df_2[df_2['start_station_id'] == '5382.07'][['start_lat', 'start_lng']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3550e666",
   "metadata": {},
   "source": [
    "Multiple pairs of latitude and longitude are tied to the same station name, but they are in close proximity, indicating that they refer to the start coordinates of trips made from the same station. In order to map the incoming and outgoing traffic from bike stations, it is easier to work with a single set of coordinates for each station. To that end, merge the `df_1` and `station_info` DataFrames. The missing station coordinates can be imputed by averaging the trip coordinates associated with each station.\n",
    "\n",
    "As proof of concept, compare the latitude and longitude of `5382.07` from the station information with the averaged trip coordinates tied to the station: they are similar down to the fourth decimal place, which implies accuracy of about 10 m in physical space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ed94649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>40.717798</td>\n",
       "      <td>-73.993161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            lat        lon\n",
       "1123  40.717798 -73.993161"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_info.loc[station_info['station_id'] == '5382.07'][['lat', 'lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98fb2f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start_lat    40.717800\n",
       "start_lng   -73.993165\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2[df_2['start_station_id'] == '5382.07'][['start_lat', 'start_lng']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bca9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare start_stations DataFrame for merging.\n",
    "start_stations = station_info.copy()\n",
    "start_stations = start_stations.loc[:, ['station_id', 'lat', 'lon']]\n",
    "start_stations.rename(columns={'station_id':'start_station_id',\n",
    "                               'lat':'start_station_lat',\n",
    "                               'lon': 'start_station_lng'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53a0f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left outer merge start_startions into df_2.\n",
    "df_2 = pd.merge(df_2, start_stations, on='start_station_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cddcd940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3441935 entries, 0 to 3441934\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count    Dtype         \n",
      "---  ------              --------------    -----         \n",
      " 0   started_at          3441935 non-null  datetime64[ns]\n",
      " 1   ended_at            3441935 non-null  datetime64[ns]\n",
      " 2   start_station_name  3441935 non-null  object        \n",
      " 3   start_station_id    3441935 non-null  object        \n",
      " 4   end_station_name    3439610 non-null  object        \n",
      " 5   end_station_id      3439610 non-null  object        \n",
      " 6   start_lat           3441935 non-null  float64       \n",
      " 7   start_lng           3441935 non-null  float64       \n",
      " 8   end_lat             3441654 non-null  float64       \n",
      " 9   end_lng             3441654 non-null  float64       \n",
      " 10  user_type           3441935 non-null  object        \n",
      " 11  start_station_lat   3354913 non-null  float64       \n",
      " 12  start_station_lng   3354913 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(6), object(5)\n",
      "memory usage: 367.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_2.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55b8f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing start station coordinates with the average trip start coordinates.\n",
    "df_2['start_station_lat'].fillna(\n",
    "    df_2[df_2['start_station_lat'].isna()].groupby('start_station_id')['start_lat'].transform('mean'), inplace=True)\n",
    "df_2['start_station_lng'].fillna(\n",
    "    df_2[df_2['start_station_lng'].isna()].groupby('start_station_id')['start_lng'].transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b964125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for end stations.\n",
    "end_stations = station_info.copy()\n",
    "end_stations = end_stations.loc[:, ['station_id', 'lat', 'lon']]\n",
    "end_stations.rename(columns={'station_id':'end_station_id', \n",
    "                             'lat':'end_station_lat', \n",
    "                             'lon': 'end_station_lng'}, inplace=True)\n",
    "\n",
    "df_2 = pd.merge(df_2, end_stations, on='end_station_id', how='left')\n",
    "\n",
    "df_2['end_station_lat'].fillna(\n",
    "    df_2[df_2['end_station_lat'].isna()].groupby('end_station_id')['end_lat'].transform('mean'), inplace=True)\n",
    "df_2['end_station_lng'].fillna(\n",
    "    df_2[df_2['end_station_lng'].isna()].groupby('end_station_id')['end_lng'].transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14c5881a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3441935 entries, 0 to 3441934\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count    Dtype         \n",
      "---  ------              --------------    -----         \n",
      " 0   started_at          3441935 non-null  datetime64[ns]\n",
      " 1   ended_at            3441935 non-null  datetime64[ns]\n",
      " 2   start_station_name  3441935 non-null  object        \n",
      " 3   start_station_id    3441935 non-null  object        \n",
      " 4   end_station_name    3439610 non-null  object        \n",
      " 5   end_station_id      3439610 non-null  object        \n",
      " 6   start_lat           3441935 non-null  float64       \n",
      " 7   start_lng           3441935 non-null  float64       \n",
      " 8   end_lat             3441654 non-null  float64       \n",
      " 9   end_lng             3441654 non-null  float64       \n",
      " 10  user_type           3441935 non-null  object        \n",
      " 11  start_station_lat   3441935 non-null  float64       \n",
      " 12  start_station_lng   3441935 non-null  float64       \n",
      " 13  end_station_lat     3439610 non-null  float64       \n",
      " 14  end_station_lng     3439610 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(8), object(5)\n",
      "memory usage: 420.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_2.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad1ff423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the trip coordinates as the unique station coordinates have been determined.\n",
    "df_2.drop(columns=['start_lat', 'start_lng', 'end_lat', 'end_lng'], inplace=True)\n",
    "\n",
    "# Drop rows without end station info as the study focuses on trips that start and end at bike stations.\n",
    "df_2.dropna(inplace=True)\n",
    "\n",
    "# Reorganize the columns.\n",
    "df_2 = df_2.iloc[:, [0,1,2,3,7,8,4,5,9,10,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfdbdf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame that summarizes the station info as a result of data wrangling documented in Station Coordinates.\n",
    "stations = df_2[['end_station_id', 'end_station_name', 'end_station_lat', 'end_station_lng']].drop_duplicates()\n",
    "stations.rename(columns={'end_station_id': 'station_id',\n",
    "                         'end_station_name': 'station_name',\n",
    "                         'end_station_lat': 'station_lat',\n",
    "                         'end_station_lng': 'station_lng'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c102a3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3439610 entries, 0 to 3441934\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count    Dtype         \n",
      "---  ------              --------------    -----         \n",
      " 0   started_at          3439610 non-null  datetime64[ns]\n",
      " 1   ended_at            3439610 non-null  datetime64[ns]\n",
      " 2   start_station_name  3439610 non-null  object        \n",
      " 3   start_station_id    3439610 non-null  object        \n",
      " 4   start_station_lat   3439610 non-null  float64       \n",
      " 5   start_station_lng   3439610 non-null  float64       \n",
      " 6   end_station_name    3439610 non-null  object        \n",
      " 7   end_station_id      3439610 non-null  object        \n",
      " 8   end_station_lat     3439610 non-null  float64       \n",
      " 9   end_station_lng     3439610 non-null  float64       \n",
      " 10  user_type           3439610 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(4), object(5)\n",
      "memory usage: 314.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_2.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5044fd2d",
   "metadata": {},
   "source": [
    "## Net Traffic\n",
    "The incoming and outgoing traffic from bike stations is of importance to monitor the balance of the system, so a separate DataFrame is created that summarizes the hourly traffic at each bike station for the time period of the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e82a1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check all start stations are in the list of end stations.\n",
    "# start_stations = df_2['start_station_id'].unique().tolist()\n",
    "# end_stations = df_2['start_station_id'].unique().tolist()\n",
    "# all(station in end_stations for station in start_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2ee0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_2.copy()\n",
    "\n",
    "# Add date and hour as additional features to group the bike station traffic by. Station id, day, hour, and user type together\n",
    "# define the level of granularity at which traffic within the bike sharing system is investigated.\n",
    "df_3 = df_3.assign(started_at_date=df_3['started_at'].dt.date, \n",
    "                   started_at_hour=df_3['started_at'].dt.hour,\n",
    "                   ended_at_date=df_3['ended_at'].dt.date,\n",
    "                   ended_at_hour=df_3['ended_at'].dt.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35fe219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the hourly outgoing traffic (i.e. number of trips started) at each station every day of the time period investigated,\n",
    "# which is June 2022.\n",
    "outgoing = df_3.groupby(['start_station_id', 'started_at_date', 'started_at_hour'])['started_at'].agg('count').reset_index()\n",
    "outgoing.rename(columns={'start_station_id':'station_id',\n",
    "                         'started_at_date':'date',\n",
    "                         'started_at_hour':'hour',\n",
    "                         'started_at':'outgoing_count'}, inplace=True)\n",
    "outgoing.set_index(['station_id', 'date', 'hour'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cafb5d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the hourly incoming traffic (i.e. number of trips ended) at each station every day of the time period investigated.\n",
    "incoming = df_3.groupby(['end_station_id', 'ended_at_date', 'ended_at_hour'])['ended_at'].agg('count').reset_index()\n",
    "incoming.rename(columns={'end_station_id':'station_id', \n",
    "                         'ended_at_date':'date', \n",
    "                         'ended_at_hour':'hour', \n",
    "                         'ended_at':'incoming_count'}, inplace=True)\n",
    "incoming.set_index(['station_id', 'date', 'hour'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f87c084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station_id  date        hour\n",
       "2733.03     2022-06-01  1       1\n",
       "                        6      -1\n",
       "                        7       1\n",
       "                        9       0\n",
       "                        11      0\n",
       "                               ..\n",
       "SYS038      2022-06-24  23     -1\n",
       "            2022-06-29  6       1\n",
       "                        15     -1\n",
       "                        19     -1\n",
       "            2022-06-30  21     -1\n",
       "Name: net_traffic, Length: 797776, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the hourly net traffic at each station every day of the time period investigated. Negative net traffic means more\n",
    "# bikes leaving than arriving within the hour. Depending on the station capacity, high negative net traffic can indicate need of\n",
    "# supply.\n",
    "net = (incoming['incoming_count']-outgoing['outgoing_count']) \\\n",
    "    .combine_first(incoming['incoming_count']).combine_first(-outgoing['outgoing_count']).astype('int64')\n",
    "net.rename('net_traffic', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3543c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station_id  date        hour\n",
       "2733.03     2022-06-01  1       1\n",
       "                        6       1\n",
       "                        7       1\n",
       "                        9       2\n",
       "                        11      2\n",
       "                               ..\n",
       "SYS038      2022-06-24  23      1\n",
       "            2022-06-29  6       1\n",
       "                        15      1\n",
       "                        19      1\n",
       "            2022-06-30  21      1\n",
       "Name: total_traffic, Length: 797776, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the hourly total traffic (i.e. sum of incoming and outgoing trips) at each station every day of the time period\n",
    "# investigated, which is a measure of a station's popularity.\n",
    "total = (incoming['incoming_count']+outgoing['outgoing_count']) \\\n",
    "    .combine_first(incoming['incoming_count']).combine_first(outgoing['outgoing_count']).astype('int64')\n",
    "total.rename('total_traffic', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94917828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the traffic info with the station info.\n",
    "df_traffic = pd.concat([net, total], axis=1).reset_index().merge(stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc73d071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>net_traffic</th>\n",
       "      <th>total_traffic</th>\n",
       "      <th>station_name</th>\n",
       "      <th>station_lat</th>\n",
       "      <th>station_lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2733.03</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67 St &amp; Erik Pl</td>\n",
       "      <td>40.633385</td>\n",
       "      <td>-74.016562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2733.03</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>67 St &amp; Erik Pl</td>\n",
       "      <td>40.633385</td>\n",
       "      <td>-74.016562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2733.03</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67 St &amp; Erik Pl</td>\n",
       "      <td>40.633385</td>\n",
       "      <td>-74.016562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2733.03</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>67 St &amp; Erik Pl</td>\n",
       "      <td>40.633385</td>\n",
       "      <td>-74.016562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2733.03</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>67 St &amp; Erik Pl</td>\n",
       "      <td>40.633385</td>\n",
       "      <td>-74.016562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id        date  hour  net_traffic  total_traffic     station_name  \\\n",
       "0    2733.03  2022-06-01     1            1              1  67 St & Erik Pl   \n",
       "1    2733.03  2022-06-01     6           -1              1  67 St & Erik Pl   \n",
       "2    2733.03  2022-06-01     7            1              1  67 St & Erik Pl   \n",
       "3    2733.03  2022-06-01     9            0              2  67 St & Erik Pl   \n",
       "4    2733.03  2022-06-01    11            0              2  67 St & Erik Pl   \n",
       "\n",
       "   station_lat  station_lng  \n",
       "0    40.633385   -74.016562  \n",
       "1    40.633385   -74.016562  \n",
       "2    40.633385   -74.016562  \n",
       "3    40.633385   -74.016562  \n",
       "4    40.633385   -74.016562  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_traffic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9473c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize the columns for export.\n",
    "df_traffic = df_traffic.iloc[:, [0,5,6,7,1,2,3,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6c5740",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa2f05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_2[['started_at', 'ended_at', 'user_type']].to_csv(os.path.join('data', '202206-citibike-tripdata-cleaned.csv'), index=False)\n",
    "# df_traffic.to_csv(os.path.join('data', '202206-citibike-citibike-station-traffic.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
